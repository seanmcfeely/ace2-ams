{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Analysis Correlation Engine - Alert Management System \u00b6 ACE2 is comprised of the Core and the Alert Management System. The AMS provides a web interface for analysts to interact with the alerts. Quick Start \u00b6 Your local system will need an entry in the hosts file to properly work with the AMS development environment. For Mac/Linux, this file is located at /etc/hosts . In Windows, this file is located at C:\\Windows\\System32\\drivers\\etc\\hosts . You will need to open Notepad or another text editor as an Administrator in order to edit the hosts file. Add the following entry to the file: 127.0.0.1 ace2-ams With your host file updated, you can use the helper script to reset and build the local AMS development environment that includes hot-reloading for both the frontend and APIs: bin/reset-dev-container.sh After the containers are built and running, you can access the components using the following URLs: Frontend: http://ace2-ams:8080 Database API Swagger documentation: http://ace2-ams:8888/docs Database API ReDoc documentation: http://ace2-ams:8888/redoc GUI API Swagger documentation: http://ace2-ams:7777/docs GUI API ReDoc documentation: http://ace2-ams:7777/redoc Philosophy \u00b6 For a more in-depth understanding of the philosophy behind ACE, see the talk that John Davison gave on the development of the ACE tool set at BSides Cincinnati in 2015.","title":"Analysis Correlation Engine - Alert Management System"},{"location":"#analysis-correlation-engine-alert-management-system","text":"ACE2 is comprised of the Core and the Alert Management System. The AMS provides a web interface for analysts to interact with the alerts.","title":"Analysis Correlation Engine - Alert Management System"},{"location":"#quick-start","text":"Your local system will need an entry in the hosts file to properly work with the AMS development environment. For Mac/Linux, this file is located at /etc/hosts . In Windows, this file is located at C:\\Windows\\System32\\drivers\\etc\\hosts . You will need to open Notepad or another text editor as an Administrator in order to edit the hosts file. Add the following entry to the file: 127.0.0.1 ace2-ams With your host file updated, you can use the helper script to reset and build the local AMS development environment that includes hot-reloading for both the frontend and APIs: bin/reset-dev-container.sh After the containers are built and running, you can access the components using the following URLs: Frontend: http://ace2-ams:8080 Database API Swagger documentation: http://ace2-ams:8888/docs Database API ReDoc documentation: http://ace2-ams:8888/redoc GUI API Swagger documentation: http://ace2-ams:7777/docs GUI API ReDoc documentation: http://ace2-ams:7777/redoc","title":"Quick Start"},{"location":"#philosophy","text":"For a more in-depth understanding of the philosophy behind ACE, see the talk that John Davison gave on the development of the ACE tool set at BSides Cincinnati in 2015.","title":"Philosophy"},{"location":"development/","text":"Setup \u00b6 Initial setup \u00b6 This project requires the use of Docker, Python (3.9.x), and Node.js (16.x). Install those as normal for your operating system. However, if you are on Windows, see the Windows section below. Windows \u00b6 If you want to run the Cypress component or end-to-end tests using the graphical test runner (instead of just from the command line), you will need to either: Ensure you have Node.js 16.x installed within Windows. OR If you are using WSL2 (see below), install an X-server in Windows so that when you run npx cypress open within WSL2 it is able to connect to the X-server to display the Cypress GUI. To do this, you can follow the instructions as described in this Stack Overflow answer . Using WSL2 (Optional) \u00b6 If you have access to WSL2, a better development experience (to not have to deal with Docker Desktop's performance issues) in Windows is to set up WSL2 with Ubuntu. Then within Ubuntu, you would install Docker, Python, and Node.js and do all of your work from there. VSCode can also be configured using the Remote - WSL extension to connect to a folder within your WSL2 instance. Updating your hosts file \u00b6 Until we figure out a better way to deal with this, your local system will need an entry in the hosts file to properly work with the AMS development environment. This is because of the combination of how the Docker containers interact with one another, CORS, and the fact that the dev environment does not use HTTPS, which carries implications for the HttpOnly cookies that are used with the frontend's authentication. For Mac/Linux, this file is located at /etc/hosts . In Windows, this file is located at C:\\Windows\\System32\\drivers\\etc\\hosts . You will need to open Notepad or another text editor as an Administrator in order to edit the hosts file. Add the following entry to the file: 127.0.0.1 ace2-ams Building behind a proxy \u00b6 If you are in an environment where a proxy is enforced then you will need to create a file in the root project directory called .env with the following contents. http_proxy=http://your.proxy:port https_proxy=http://your.proxy:port If your proxy breaks SSL (if you are getting some kind of an SSL error) then you will also need to ignore SSL warnings. http_proxy=http://your.proxy:port https_proxy=http://your.proxy:port pip_install_options=--trusted-host pypi.org --trusted-host files.pythonhosted.org npm_strict_ssl=false Starting the application \u00b6 You can start the application using Docker containers so that it uses hot-reloading anytime you change a file: bin/reset-dev-container.sh This script will generate random passwords for the database user and the secret key used for JWTs. If you need to access these, you can view them in the $HOME/.ace2.env file, which configures the environment variables that will be loaded into the database container. Once the both the frontend and API development environments are built and started, you can access the components: Frontend: http://ace2-ams:8080 Database API Swagger documentation: http://ace2-ams:8888/docs Database API ReDoc documentation: http://ace2-ams:8888/redoc GUI API Swagger documentation: http://ace2-ams:7777/docs GUI API ReDoc documentation: http://ace2-ams:7777/redoc VSCode extensions \u00b6 If you use VSCode, the following extensions are useful for working with the project: ESLint Prettier - Code formatter Pylance Python Vue Language Features (Volar) Once you install Volar, you will want to enable its \"takeover\" mode. To do that, you need to disable VSCode's built-in TypeScript extension: In the extensions panel, search for @builtin typescript . Right-click on the TypeScript and JavaScript Language Features extension and select Disable (Workspace) . This will allow Volar to provide TypeScript support as well as proper support for working with Vue's single file components. Managing NPM packages \u00b6 You should not directly edit the dependencies or devDependencies inside of package.json or anything in package-lock.json . Any changes to packages should be performed via the npm command : Install new dependency package \u00b6 You would install a package like this if it is something the final compiled application needs: npm install <package> Install new dev dependency package \u00b6 You would install a package like this if it is only needed during development: npm install -D <package> Uninstall package \u00b6 You can uninstall/remove a package regardless of how it was installed by: npm uninstall <package> Running tests \u00b6 Database API \u00b6 The database API has a suite of tests performed by Pytest that includes code coverage: bin/test-db-api.sh You can run a specific portion of the tests using the same script: bin/test-db-api.sh db_api/app/tests/api/test_auth.py GUI API \u00b6 The GUI API has a suite of tests performed by Pytest that includes code coverage: bin/test-gui-api.sh You can run a specific portion of the tests using the same script: bin/test-gui-api.sh gui_api/app/tests/api/test_auth.py Frontend \u00b6 This frontend has a suite of unit tests performed by Vitest and component and end-to-end tests performed by Cypress . Unit tests \u00b6 You can execute the unit tests by running: bin/test-frontend-unit.sh Component Tests \u00b6 You can execute the end-to-end tests by running: bin/test-components.sh End-to-end tests \u00b6 You can execute the end-to-end tests by running: bin/test-e2e.sh Test Runner \u00b6 Cypress also comes with an amazing Test Runner that lets you see and interact with the tests in your local web browser. This can be helpful when writing component and end-to-end tests to ensure they are working properly as well as any debugging you might need to do. However, this will need to be performed on your local system ouside of the containers (see the Windows setup section in case you are using WSL2). To do this, you will need to have Node.js 16 installed. Step 1: Prep the application to run the tests in interactive mode (this tells the containers to use the test database): bin/test-interactive-e2e.sh Step 2: Install the Node.js packages on your host system (this only needs to be done one time or when package.json is updated): cd frontend/ npm install Step 3: Open the Test Runner on your host system: cd frontend/ npx cypress open For more information on what you can do with the Test Runner, view the Test Runner documentation . Step 4: Disable testing mode once you are finished using the Test Runner: bin/disable-test-mode.sh","title":"Setup"},{"location":"development/#setup","text":"","title":"Setup"},{"location":"development/#initial-setup","text":"This project requires the use of Docker, Python (3.9.x), and Node.js (16.x). Install those as normal for your operating system. However, if you are on Windows, see the Windows section below.","title":"Initial setup"},{"location":"development/#windows","text":"If you want to run the Cypress component or end-to-end tests using the graphical test runner (instead of just from the command line), you will need to either: Ensure you have Node.js 16.x installed within Windows. OR If you are using WSL2 (see below), install an X-server in Windows so that when you run npx cypress open within WSL2 it is able to connect to the X-server to display the Cypress GUI. To do this, you can follow the instructions as described in this Stack Overflow answer .","title":"Windows"},{"location":"development/#using-wsl2-optional","text":"If you have access to WSL2, a better development experience (to not have to deal with Docker Desktop's performance issues) in Windows is to set up WSL2 with Ubuntu. Then within Ubuntu, you would install Docker, Python, and Node.js and do all of your work from there. VSCode can also be configured using the Remote - WSL extension to connect to a folder within your WSL2 instance.","title":"Using WSL2 (Optional)"},{"location":"development/#updating-your-hosts-file","text":"Until we figure out a better way to deal with this, your local system will need an entry in the hosts file to properly work with the AMS development environment. This is because of the combination of how the Docker containers interact with one another, CORS, and the fact that the dev environment does not use HTTPS, which carries implications for the HttpOnly cookies that are used with the frontend's authentication. For Mac/Linux, this file is located at /etc/hosts . In Windows, this file is located at C:\\Windows\\System32\\drivers\\etc\\hosts . You will need to open Notepad or another text editor as an Administrator in order to edit the hosts file. Add the following entry to the file: 127.0.0.1 ace2-ams","title":"Updating your hosts file"},{"location":"development/#building-behind-a-proxy","text":"If you are in an environment where a proxy is enforced then you will need to create a file in the root project directory called .env with the following contents. http_proxy=http://your.proxy:port https_proxy=http://your.proxy:port If your proxy breaks SSL (if you are getting some kind of an SSL error) then you will also need to ignore SSL warnings. http_proxy=http://your.proxy:port https_proxy=http://your.proxy:port pip_install_options=--trusted-host pypi.org --trusted-host files.pythonhosted.org npm_strict_ssl=false","title":"Building behind a proxy"},{"location":"development/#starting-the-application","text":"You can start the application using Docker containers so that it uses hot-reloading anytime you change a file: bin/reset-dev-container.sh This script will generate random passwords for the database user and the secret key used for JWTs. If you need to access these, you can view them in the $HOME/.ace2.env file, which configures the environment variables that will be loaded into the database container. Once the both the frontend and API development environments are built and started, you can access the components: Frontend: http://ace2-ams:8080 Database API Swagger documentation: http://ace2-ams:8888/docs Database API ReDoc documentation: http://ace2-ams:8888/redoc GUI API Swagger documentation: http://ace2-ams:7777/docs GUI API ReDoc documentation: http://ace2-ams:7777/redoc","title":"Starting the application"},{"location":"development/#vscode-extensions","text":"If you use VSCode, the following extensions are useful for working with the project: ESLint Prettier - Code formatter Pylance Python Vue Language Features (Volar) Once you install Volar, you will want to enable its \"takeover\" mode. To do that, you need to disable VSCode's built-in TypeScript extension: In the extensions panel, search for @builtin typescript . Right-click on the TypeScript and JavaScript Language Features extension and select Disable (Workspace) . This will allow Volar to provide TypeScript support as well as proper support for working with Vue's single file components.","title":"VSCode extensions"},{"location":"development/#managing-npm-packages","text":"You should not directly edit the dependencies or devDependencies inside of package.json or anything in package-lock.json . Any changes to packages should be performed via the npm command :","title":"Managing NPM packages"},{"location":"development/#install-new-dependency-package","text":"You would install a package like this if it is something the final compiled application needs: npm install <package>","title":"Install new dependency package"},{"location":"development/#install-new-dev-dependency-package","text":"You would install a package like this if it is only needed during development: npm install -D <package>","title":"Install new dev dependency package"},{"location":"development/#uninstall-package","text":"You can uninstall/remove a package regardless of how it was installed by: npm uninstall <package>","title":"Uninstall package"},{"location":"development/#running-tests","text":"","title":"Running tests"},{"location":"development/#database-api","text":"The database API has a suite of tests performed by Pytest that includes code coverage: bin/test-db-api.sh You can run a specific portion of the tests using the same script: bin/test-db-api.sh db_api/app/tests/api/test_auth.py","title":"Database API"},{"location":"development/#gui-api","text":"The GUI API has a suite of tests performed by Pytest that includes code coverage: bin/test-gui-api.sh You can run a specific portion of the tests using the same script: bin/test-gui-api.sh gui_api/app/tests/api/test_auth.py","title":"GUI API"},{"location":"development/#frontend","text":"This frontend has a suite of unit tests performed by Vitest and component and end-to-end tests performed by Cypress .","title":"Frontend"},{"location":"development/#unit-tests","text":"You can execute the unit tests by running: bin/test-frontend-unit.sh","title":"Unit tests"},{"location":"development/#component-tests","text":"You can execute the end-to-end tests by running: bin/test-components.sh","title":"Component Tests"},{"location":"development/#end-to-end-tests","text":"You can execute the end-to-end tests by running: bin/test-e2e.sh","title":"End-to-end tests"},{"location":"development/#test-runner","text":"Cypress also comes with an amazing Test Runner that lets you see and interact with the tests in your local web browser. This can be helpful when writing component and end-to-end tests to ensure they are working properly as well as any debugging you might need to do. However, this will need to be performed on your local system ouside of the containers (see the Windows setup section in case you are using WSL2). To do this, you will need to have Node.js 16 installed. Step 1: Prep the application to run the tests in interactive mode (this tells the containers to use the test database): bin/test-interactive-e2e.sh Step 2: Install the Node.js packages on your host system (this only needs to be done one time or when package.json is updated): cd frontend/ npm install Step 3: Open the Test Runner on your host system: cd frontend/ npx cypress open For more information on what you can do with the Test Runner, view the Test Runner documentation . Step 4: Disable testing mode once you are finished using the Test Runner: bin/disable-test-mode.sh","title":"Test Runner"},{"location":"development/backend/database/","text":"Database \u00b6 Schema definitions \u00b6 The database tables used by the database API are all defined using SQLAlchemy. The table models are stored in db_api/app/db/models.py . Alembic \u00b6 Creating revisions \u00b6 The models are applied to the database using Alembic . Whenever you make any changes to the database models, you will need to generate a new Alembic \"revision\". You can use the bin/db-revision.sh script to help create a new revision after you've made some changes to the database models: bin/db-revision.sh \"Some short note\" This script uses Alembic's \"autogenerate\" feature to create what it thinks is the correct database migration script for the changes you made. Autogenerate is not always perfect , so you should always verify the migration script it creates before committing it to the repo (and applying it in production) to make sure what will be applied to the database is correct. The migration scripts can be found in db_api/db/migrations/versions/ . An example migration script for creating the \"tag\" database table is shown below: \"\"\"Initial revision Revision ID: 03fe4895893f Revises: Create Date: 2021-04-16 21:08:29.237142 \"\"\" from alembic import op import sqlalchemy as sa # revision identifiers, used by Alembic revision = '03fe4895893f' down_revision = None branch_labels = None depends_on = None def upgrade () -> None : # ### commands auto generated by Alembic - please adjust! ### op . create_table ( 'tag' , sa . Column ( 'id' , sa . Integer (), nullable = False ), sa . Column ( 'name' , sa . String (), nullable = True ), sa . PrimaryKeyConstraint ( 'id' ) ) op . create_index ( op . f ( 'ix_tag_id' ), 'tag' , [ 'id' ], unique = False ) # ### end Alembic commands ### def downgrade () -> None : # ### commands auto generated by Alembic - please adjust! ### op . drop_index ( op . f ( 'ix_tag_id' ), table_name = 'tag' ) op . drop_table ( 'tag' ) # ### end Alembic commands ### This script shows you what it will do when you issue the \"upgrade\" command to the database as well as if you need to revert and issue the \"downgrade\" command. You can manually edit this migration script if something is incorrect. Applying revisions \u00b6 Once you have a new revision you'd like to apply to the database, you can either use the bin/reset-dev-container.sh script to rebuild your entire development environment (which will automatically apply the Alembic database revisions), or you can use the bin/db-upgrade.sh script to apply the revisions without erasing and rebuilding the development environment.","title":"Database"},{"location":"development/backend/database/#database","text":"","title":"Database"},{"location":"development/backend/database/#schema-definitions","text":"The database tables used by the database API are all defined using SQLAlchemy. The table models are stored in db_api/app/db/models.py .","title":"Schema definitions"},{"location":"development/backend/database/#alembic","text":"","title":"Alembic"},{"location":"development/backend/database/#creating-revisions","text":"The models are applied to the database using Alembic . Whenever you make any changes to the database models, you will need to generate a new Alembic \"revision\". You can use the bin/db-revision.sh script to help create a new revision after you've made some changes to the database models: bin/db-revision.sh \"Some short note\" This script uses Alembic's \"autogenerate\" feature to create what it thinks is the correct database migration script for the changes you made. Autogenerate is not always perfect , so you should always verify the migration script it creates before committing it to the repo (and applying it in production) to make sure what will be applied to the database is correct. The migration scripts can be found in db_api/db/migrations/versions/ . An example migration script for creating the \"tag\" database table is shown below: \"\"\"Initial revision Revision ID: 03fe4895893f Revises: Create Date: 2021-04-16 21:08:29.237142 \"\"\" from alembic import op import sqlalchemy as sa # revision identifiers, used by Alembic revision = '03fe4895893f' down_revision = None branch_labels = None depends_on = None def upgrade () -> None : # ### commands auto generated by Alembic - please adjust! ### op . create_table ( 'tag' , sa . Column ( 'id' , sa . Integer (), nullable = False ), sa . Column ( 'name' , sa . String (), nullable = True ), sa . PrimaryKeyConstraint ( 'id' ) ) op . create_index ( op . f ( 'ix_tag_id' ), 'tag' , [ 'id' ], unique = False ) # ### end Alembic commands ### def downgrade () -> None : # ### commands auto generated by Alembic - please adjust! ### op . drop_index ( op . f ( 'ix_tag_id' ), table_name = 'tag' ) op . drop_table ( 'tag' ) # ### end Alembic commands ### This script shows you what it will do when you issue the \"upgrade\" command to the database as well as if you need to revert and issue the \"downgrade\" command. You can manually edit this migration script if something is incorrect.","title":"Creating revisions"},{"location":"development/backend/database/#applying-revisions","text":"Once you have a new revision you'd like to apply to the database, you can either use the bin/reset-dev-container.sh script to rebuild your entire development environment (which will automatically apply the Alembic database revisions), or you can use the bin/db-upgrade.sh script to apply the revisions without erasing and rebuilding the development environment.","title":"Applying revisions"},{"location":"development/backend/environment_variables/","text":"Environment Variables \u00b6 The backend container relies on a number of environment variables. For the development environment, these are automatically generated by the bin/reset-dev-container.sh script and stored inside of the .ace2.env file inside of your home directory. These environment variables will need to be set by other means if you are running this application in production. Database API variables \u00b6 These variables are used by the database API application. ACE_DEV : If set (to anything), the application will run in development-mode, which means that the Alembic database migrations will be applied and the database seeded with basic information automatically when the application starts. This is enabled by default for the development environment. DATABASE_URL : The connection string used to connect to the PostgreSQL server. It should be in the form of postgresql://user:password@hostname[:port]/dbname . DATABASE_TEST_URL : The connection string used to connect to the test PostgreSQL server. It should be in the form of postgresql://user:password@hostname[:port]/dbname . DEFAULT_ANALYSIS_MODE_ALERT : The alert analysis mode to use if one is not given when creating a submission. DEFAULT_ANALYSIS_MODE_DETECT : The detect analysis mode to use if one is not given when creating a submission. DEFAULT_ANALYSIS_MODE_EVENT : The event analysis mode to use if one is not given when creating a submission. DEFAULT_ANALYSIS_MODE_RESPONSE : The response analysis mode to use if one is not given when creating a submission. SQL_ECHO : If set (to anything), SQLAlchemy will be configured to echo all queries to the console. You can view the queries in the Docker logs for the ace2-ams-api container. This is enabled by default for the development environment. TESTING : If set to \"yes\", this instructs the API to utilize the test database (such as when running the unit tests). GUI API variables \u00b6 These variables are used by the GUI API application. COOKIES_SAMESITE : The SameSite value to use when sending cookies. The development environment uses lax . Defaults to lax . COOKIES_SECURE : True/False whether or not you want to require HTTPS when sending cookies. The development environment uses False . Defaults to True . DATABASE_API_URL : The base URL to reach the database API. The development environment uses http://db-api/api by default. JWT_ACCESS_EXPIRE_SECONDS : The number of seconds after which an access token will expire. The development environment uses 900 (15 minutes) by default. JWT_ALGORITHM : Sets the algorithm to use for signing the tokens. The development environment uses HS256 by default. JWT_REFRESH_EXPIRE_SECONDS : The number of seconds after which a refresh token will expire. The development environment uses 43200 (12 hours) by default. JWT_SECRET : The secret key/password to use when signing and decoding tokens. The development environment generates a random 32 character string . SQL_ECHO : If set (to anything), SQLAlchemy will be configured to echo all queries to the console. You can view the queries in the Docker logs for the ace2-ams-api container. This is enabled by default for the development environment. PostgreSQL container variables \u00b6 These variables are used by the PostgreSQL server container to initialize the database. POSTGRES_DB : The name of the database to create. The development environment uses ace . POSTGRES_USER : The user to use to connect to the PostgreSQL server. The development environment uses ace . POSTGRES_PASSWORD : The password to use to connect to the PostgreSQL server. The development environment generates a random 32 character string .","title":"Environment Variables"},{"location":"development/backend/environment_variables/#environment-variables","text":"The backend container relies on a number of environment variables. For the development environment, these are automatically generated by the bin/reset-dev-container.sh script and stored inside of the .ace2.env file inside of your home directory. These environment variables will need to be set by other means if you are running this application in production.","title":"Environment Variables"},{"location":"development/backend/environment_variables/#database-api-variables","text":"These variables are used by the database API application. ACE_DEV : If set (to anything), the application will run in development-mode, which means that the Alembic database migrations will be applied and the database seeded with basic information automatically when the application starts. This is enabled by default for the development environment. DATABASE_URL : The connection string used to connect to the PostgreSQL server. It should be in the form of postgresql://user:password@hostname[:port]/dbname . DATABASE_TEST_URL : The connection string used to connect to the test PostgreSQL server. It should be in the form of postgresql://user:password@hostname[:port]/dbname . DEFAULT_ANALYSIS_MODE_ALERT : The alert analysis mode to use if one is not given when creating a submission. DEFAULT_ANALYSIS_MODE_DETECT : The detect analysis mode to use if one is not given when creating a submission. DEFAULT_ANALYSIS_MODE_EVENT : The event analysis mode to use if one is not given when creating a submission. DEFAULT_ANALYSIS_MODE_RESPONSE : The response analysis mode to use if one is not given when creating a submission. SQL_ECHO : If set (to anything), SQLAlchemy will be configured to echo all queries to the console. You can view the queries in the Docker logs for the ace2-ams-api container. This is enabled by default for the development environment. TESTING : If set to \"yes\", this instructs the API to utilize the test database (such as when running the unit tests).","title":"Database API variables"},{"location":"development/backend/environment_variables/#gui-api-variables","text":"These variables are used by the GUI API application. COOKIES_SAMESITE : The SameSite value to use when sending cookies. The development environment uses lax . Defaults to lax . COOKIES_SECURE : True/False whether or not you want to require HTTPS when sending cookies. The development environment uses False . Defaults to True . DATABASE_API_URL : The base URL to reach the database API. The development environment uses http://db-api/api by default. JWT_ACCESS_EXPIRE_SECONDS : The number of seconds after which an access token will expire. The development environment uses 900 (15 minutes) by default. JWT_ALGORITHM : Sets the algorithm to use for signing the tokens. The development environment uses HS256 by default. JWT_REFRESH_EXPIRE_SECONDS : The number of seconds after which a refresh token will expire. The development environment uses 43200 (12 hours) by default. JWT_SECRET : The secret key/password to use when signing and decoding tokens. The development environment generates a random 32 character string . SQL_ECHO : If set (to anything), SQLAlchemy will be configured to echo all queries to the console. You can view the queries in the Docker logs for the ace2-ams-api container. This is enabled by default for the development environment.","title":"GUI API variables"},{"location":"development/backend/environment_variables/#postgresql-container-variables","text":"These variables are used by the PostgreSQL server container to initialize the database. POSTGRES_DB : The name of the database to create. The development environment uses ace . POSTGRES_USER : The user to use to connect to the PostgreSQL server. The development environment uses ace . POSTGRES_PASSWORD : The password to use to connect to the PostgreSQL server. The development environment generates a random 32 character string .","title":"PostgreSQL container variables"},{"location":"development/backend/insert_alerts/","text":"Insert Alerts \u00b6 There is a script at bin/insert-alerts.sh that allows you to insert alerts into the database that you then view in the GUI. Basic usage \u00b6 To insert a single alert into the database: bin/insert-alerts.sh db_api/app/tests/alerts/alert.json You must pass a path to a JSON file that represents the structure of an alert's analysis and observable instance objects. There are already some example JSON files available: backdb_apiend/app/tests/alerts/large.json db_api/app/tests/alerts/small.json Adding multiple alerts \u00b6 You can use the script to add multiple alerts using the same JSON file. This will add 10 separate alerts to the database: bin/insert-alerts.sh db_api/app/tests/alerts/small.json 10 Using a dynamic alert template \u00b6 You can also use an alert template where certain tokens inside of the JSON file get replaced with dynamic/random data. An example alert template is available at db_api/app/tests/alerts/small_template.json . This template uses the following tokens that get replaced with random data using Faker: <ALERT_NAME> - A random name for the alert <A_TYPE> - Three random words used for the analysis module type <O_TYPE> - A random word used for the observable type <O_VALUE> - Two random words used for the observable value <TAG> - A random word used for a tag Using the dynamic template is the same command. To add 10 dynamic alerts to the database: bin/insert-alerts.sh db_api/app/tests/alerts/small_template.json 10","title":"Insert Alerts"},{"location":"development/backend/insert_alerts/#insert-alerts","text":"There is a script at bin/insert-alerts.sh that allows you to insert alerts into the database that you then view in the GUI.","title":"Insert Alerts"},{"location":"development/backend/insert_alerts/#basic-usage","text":"To insert a single alert into the database: bin/insert-alerts.sh db_api/app/tests/alerts/alert.json You must pass a path to a JSON file that represents the structure of an alert's analysis and observable instance objects. There are already some example JSON files available: backdb_apiend/app/tests/alerts/large.json db_api/app/tests/alerts/small.json","title":"Basic usage"},{"location":"development/backend/insert_alerts/#adding-multiple-alerts","text":"You can use the script to add multiple alerts using the same JSON file. This will add 10 separate alerts to the database: bin/insert-alerts.sh db_api/app/tests/alerts/small.json 10","title":"Adding multiple alerts"},{"location":"development/backend/insert_alerts/#using-a-dynamic-alert-template","text":"You can also use an alert template where certain tokens inside of the JSON file get replaced with dynamic/random data. An example alert template is available at db_api/app/tests/alerts/small_template.json . This template uses the following tokens that get replaced with random data using Faker: <ALERT_NAME> - A random name for the alert <A_TYPE> - Three random words used for the analysis module type <O_TYPE> - A random word used for the observable type <O_VALUE> - Two random words used for the observable value <TAG> - A random word used for a tag Using the dynamic template is the same command. To add 10 dynamic alerts to the database: bin/insert-alerts.sh db_api/app/tests/alerts/small_template.json 10","title":"Using a dynamic alert template"},{"location":"development/frontend/testing/","text":"Running Tests \u00b6 Please see the Running Tests section for further details on running tests. Unit Tests \u00b6 What to test \u00b6 Unit tests should be written for any TS/JS code: Pinia stores Services, such as API services Helper/validator functions Testing Pinia Stores \u00b6 ToDO Testing Services \u00b6 ToDo Testing Helpers \u00b6 ToDo Component Tests \u00b6 What & How to Test \u00b6 Basic Guidelines \u00b6 Do: Create a respective test file for each Single-File Component MyComponent.vue \u2192 MyComponent.spec.ts Write test cases for each possible application state Blank state - when the component is freshly loaded Interactive state - when elements of the component are interacted with, and the component's behavior in response Error state - when things go wrong (invalid input, API errors, etc.) Use a factory function to create a freshly mounted instance of the component under test in each test Write test cases for all (valid) prop-type combinations Write test cases for varying (valid) initial store states, if the component accesses any Pinia stores Think like a user when writing tests -- try to select elements by their visible text first Use the following precedence when choosing how to select an element (note: some PrimeVue elements may make this difficult, so this is just a guideline): cy.contains(\"Desired Element Text\") cy.get('[data-cy=desired-element-prop]') cy.get('[name=desired-element-name]') cy.get('#desired-element-id') cy.get('.desired-element-class) // Avoid as much as possible cy.get('button') // Avoid at all costs Do Not: Test the internals of a component, e.g., any functions within the <setup> tags Test child component functionality/presentation where it can be avoided Test a component mounted with invalid props (this should not happen IRL) Write deterministic tests Tests that rely on current date or time Tests that rely on API availability Tests that rely on any \"random\" value Mocking \u00b6 Because components pull in data from nearly every part of the application (Pinia stores, API services, configuration, helpers, 'external' data from backend, etc.), a lot of mocking and stubbing is required to make sure components are tested in isolation. Below are some quick explanations of how to mock or stub some of those pieces, as well as examples in existing tests. Cypress component test are built off of vue-test-utils in order to mount components. Examples and more detailed explanations of the below can be found here . Component Mounting Options \u00b6 Because the Cypress Component Test Runner is built off of vue-test-utils, any of the mounting options available through vue-test-utils will also be available to use in component tests. The most common in this project are the following: global/directives Some components in Vue use 'directives,' which HTML attributes that tell the component to do something (focus, blur, appear on hover, etc.) Because directives are registered at the base App-level, we have to define them in this section of component mounting The most common directive currently in use is the 'ToolTip' -- watch out for strange errors from components that use this directive! Example: mount(MyComponent, { global: { directives: { tooltip: Tooltip } }, }); global/plugins This is where we can register our various plugins such as PrimeVue, our router, and Pinia See Mocking Pinia below for more details on setting up mock Pinia data Examples: // Component does not use Pinia mount(MyComponent, { global: { plugins: [ PrimeVue, router, ], }, }); // Component uses Pinia, but doesn't require any special configuration mount(MyComponent, { global: { plugins: [ PrimeVue, createPinia(), router, ], }, }); // Component uses Pinia, and will require special configuration mount(MyComponent, { global: { plugins: [ PrimeVue, createCustomCypressPinia(testingOptions), router, ], }, }); global/provide This is where any data that is injected in the component under test can be mocked out Note: If the test is injecting some form of configuration, make sure to import the data from testConfiguration Example: mount(MyComponent, { global: { provide: { exampleProvide: \"stubData\", exampleConfig: testConfiguration.exampleConfig, }, }, }); propsData This is where any data provided as props to the component under test can be mocked out This is the mounting option most likely to change from test to test Creating an interface for a given component's props can make it easier to confidently put together various props Example: AlertTableCell.spec.ts interface myComponentProps { name: string } const defaultProps: myComponentProps = { name: \"TestName\" } mount(MyComponent, { propsData: defaultProps }); slots This is where custom slot templates can be given to components with any number of slots Example: mount(MyComponent, { slots: { header: '<h1>HEADER</h1>', footer: '<div>FOOTER</div>', }, }); Mocking Test Data \u00b6 Mocking data, either in the form of props, injected data, or data returned from API calls is crucial. Wherever possible use the mock object factories provided in the mocks/** directory. This will make sure that whatever mock data that is used aligns with its respective data model. For data that will not be commonly re-used and does not have associated factory functions, feel free to create one-off mocks in the given test file. See SandboxAnalysis.spec.ts for an example. Mocking Pinia \u00b6 When to use CreatePinia() A component uses Pinia but actions/getters/state do not need to be mocked (i.e. don't rely on outside data) Example: a component uses the ModalStore to open/close modals , in which you want to test that clicking a Modal's button will open the modal. When to use CreateCustomCypressPinia() A component using pinia needs to have an initial state. Example: TheAlertSummary pulls details from the open alert in the alertStore A component using pinia needs to have pinia actions/getters stubbed out Because of limitations between how TestingPinia and Cypress stubs and spies work, pinia actions/getters cannot be stubbed directly by name If stubActions is set to true (default) in createCustomCypressPinia , Cypress stubs will be used, which do NOT call functions being stubbed If stubActions is set to false in createCustomCypressPinia , Cypress spies will be used, which DO call functions being spied on If you want to verify the callCount/arguments with which a pinia function was called, you can do so as below: mount(MyComponent, { global: { plugins: [createCustomCypressPinia(), PrimeVue, router], } }); cy.get(\"@stub-1\").should(\"have.been.calledWith\", expectedArgs); Stubs will be accessed like stub-# , the number being the order in which the method was called. Definitely not ideal , but again this is how it works based on a limitation with testingPinia . Mocking API Calls \u00b6 Mock API calls by stubbing out the API service call rather than using nock or trying to respond dynamically to the request. Example: // Example of failed API request cy.stub(Event, \"read\") .as(\"fetchEvent\") // Alias to access this request later .rejects(new Error(\"Request failed with status code 404\")); // Example of successful API request cy.stub(Event, \"read\") .withArgs(\"uuid\") // Specify expected arguments here .as(\"fetchEvent\") .resolves(eventStub); More detailed examples in following tests: AnalyzeAlertForm.spec.ts EventAlertsTable.spec.ts Testing Strategies \u00b6 Some things you might want to test might not be very intuitive using Cypress. Testing Links \u00b6 If a component dynamically generates a link, you might want to test that said link was correctly generated and attached to the element. You can easily do this by doing the following, rather than clicking on the element and checking the resulting window location. cy.contains(\"Click Me to Go To New Page!\") .invoke(\"attr\", \"href\") .should(\"contain\", \"/my/page\"); // check for the dynamically generated link here Testing Time-Dependent Data \u00b6 Component tests are set-up to automatically run in the Eastern/New_York timezone, no matter where the tests are actually being ran from. Make sure to consider that as some datetimes are reformatted to be in the browser's timezone. In some cases, the current time will be auto-filled into forms. In order to check that these values are set correctly, cy.clock can be used to mock the current time, so that this value will be the same whenever tests are ran. Example: // Using UTC for the test time will make it easier to convert where necessary const testTime = new Date(Date.UTC(2022, 2, 29, 12, 0, 0, 0)).getTime(); describe(\"My Test Suite\", () => { it(\"My Test\", () => { cy.clock(testTime); // Rest of test... }); }); Testing for Child Components \u00b6 For the most part, you won't need to test for child components, however, in some cases you may want to access child components in order to verify that they were rendered. Example: EmailAnalysis.vue , is really just a container for its child components. Example of how to check for child components: import ChildComponent from \"@/components/ChildComponent.vue\"; import ParentComponent from \"@/components/ParentComponent.vue\"; mount(ParentComponent).then((wrapper) => { expect(wrapper.getComponent(ChildComponent)).to.exist; }); Testing for Event (reaction and emitted) \u00b6 TBU (once I get better examples :)) End-to-End Tests \u00b6 ToDo","title":"Testing"},{"location":"development/frontend/testing/#running-tests","text":"Please see the Running Tests section for further details on running tests.","title":"Running Tests"},{"location":"development/frontend/testing/#unit-tests","text":"","title":"Unit Tests"},{"location":"development/frontend/testing/#what-to-test","text":"Unit tests should be written for any TS/JS code: Pinia stores Services, such as API services Helper/validator functions","title":"What to test"},{"location":"development/frontend/testing/#testing-pinia-stores","text":"ToDO","title":"Testing Pinia Stores"},{"location":"development/frontend/testing/#testing-services","text":"ToDo","title":"Testing Services"},{"location":"development/frontend/testing/#testing-helpers","text":"ToDo","title":"Testing Helpers"},{"location":"development/frontend/testing/#component-tests","text":"","title":"Component Tests"},{"location":"development/frontend/testing/#what-how-to-test","text":"","title":"What &amp; How to Test"},{"location":"development/frontend/testing/#basic-guidelines","text":"Do: Create a respective test file for each Single-File Component MyComponent.vue \u2192 MyComponent.spec.ts Write test cases for each possible application state Blank state - when the component is freshly loaded Interactive state - when elements of the component are interacted with, and the component's behavior in response Error state - when things go wrong (invalid input, API errors, etc.) Use a factory function to create a freshly mounted instance of the component under test in each test Write test cases for all (valid) prop-type combinations Write test cases for varying (valid) initial store states, if the component accesses any Pinia stores Think like a user when writing tests -- try to select elements by their visible text first Use the following precedence when choosing how to select an element (note: some PrimeVue elements may make this difficult, so this is just a guideline): cy.contains(\"Desired Element Text\") cy.get('[data-cy=desired-element-prop]') cy.get('[name=desired-element-name]') cy.get('#desired-element-id') cy.get('.desired-element-class) // Avoid as much as possible cy.get('button') // Avoid at all costs Do Not: Test the internals of a component, e.g., any functions within the <setup> tags Test child component functionality/presentation where it can be avoided Test a component mounted with invalid props (this should not happen IRL) Write deterministic tests Tests that rely on current date or time Tests that rely on API availability Tests that rely on any \"random\" value","title":"Basic Guidelines"},{"location":"development/frontend/testing/#mocking","text":"Because components pull in data from nearly every part of the application (Pinia stores, API services, configuration, helpers, 'external' data from backend, etc.), a lot of mocking and stubbing is required to make sure components are tested in isolation. Below are some quick explanations of how to mock or stub some of those pieces, as well as examples in existing tests. Cypress component test are built off of vue-test-utils in order to mount components. Examples and more detailed explanations of the below can be found here .","title":"Mocking"},{"location":"development/frontend/testing/#component-mounting-options","text":"Because the Cypress Component Test Runner is built off of vue-test-utils, any of the mounting options available through vue-test-utils will also be available to use in component tests. The most common in this project are the following: global/directives Some components in Vue use 'directives,' which HTML attributes that tell the component to do something (focus, blur, appear on hover, etc.) Because directives are registered at the base App-level, we have to define them in this section of component mounting The most common directive currently in use is the 'ToolTip' -- watch out for strange errors from components that use this directive! Example: mount(MyComponent, { global: { directives: { tooltip: Tooltip } }, }); global/plugins This is where we can register our various plugins such as PrimeVue, our router, and Pinia See Mocking Pinia below for more details on setting up mock Pinia data Examples: // Component does not use Pinia mount(MyComponent, { global: { plugins: [ PrimeVue, router, ], }, }); // Component uses Pinia, but doesn't require any special configuration mount(MyComponent, { global: { plugins: [ PrimeVue, createPinia(), router, ], }, }); // Component uses Pinia, and will require special configuration mount(MyComponent, { global: { plugins: [ PrimeVue, createCustomCypressPinia(testingOptions), router, ], }, }); global/provide This is where any data that is injected in the component under test can be mocked out Note: If the test is injecting some form of configuration, make sure to import the data from testConfiguration Example: mount(MyComponent, { global: { provide: { exampleProvide: \"stubData\", exampleConfig: testConfiguration.exampleConfig, }, }, }); propsData This is where any data provided as props to the component under test can be mocked out This is the mounting option most likely to change from test to test Creating an interface for a given component's props can make it easier to confidently put together various props Example: AlertTableCell.spec.ts interface myComponentProps { name: string } const defaultProps: myComponentProps = { name: \"TestName\" } mount(MyComponent, { propsData: defaultProps }); slots This is where custom slot templates can be given to components with any number of slots Example: mount(MyComponent, { slots: { header: '<h1>HEADER</h1>', footer: '<div>FOOTER</div>', }, });","title":"Component Mounting Options"},{"location":"development/frontend/testing/#mocking-test-data","text":"Mocking data, either in the form of props, injected data, or data returned from API calls is crucial. Wherever possible use the mock object factories provided in the mocks/** directory. This will make sure that whatever mock data that is used aligns with its respective data model. For data that will not be commonly re-used and does not have associated factory functions, feel free to create one-off mocks in the given test file. See SandboxAnalysis.spec.ts for an example.","title":"Mocking Test Data"},{"location":"development/frontend/testing/#mocking-pinia","text":"When to use CreatePinia() A component uses Pinia but actions/getters/state do not need to be mocked (i.e. don't rely on outside data) Example: a component uses the ModalStore to open/close modals , in which you want to test that clicking a Modal's button will open the modal. When to use CreateCustomCypressPinia() A component using pinia needs to have an initial state. Example: TheAlertSummary pulls details from the open alert in the alertStore A component using pinia needs to have pinia actions/getters stubbed out Because of limitations between how TestingPinia and Cypress stubs and spies work, pinia actions/getters cannot be stubbed directly by name If stubActions is set to true (default) in createCustomCypressPinia , Cypress stubs will be used, which do NOT call functions being stubbed If stubActions is set to false in createCustomCypressPinia , Cypress spies will be used, which DO call functions being spied on If you want to verify the callCount/arguments with which a pinia function was called, you can do so as below: mount(MyComponent, { global: { plugins: [createCustomCypressPinia(), PrimeVue, router], } }); cy.get(\"@stub-1\").should(\"have.been.calledWith\", expectedArgs); Stubs will be accessed like stub-# , the number being the order in which the method was called. Definitely not ideal , but again this is how it works based on a limitation with testingPinia .","title":"Mocking Pinia"},{"location":"development/frontend/testing/#mocking-api-calls","text":"Mock API calls by stubbing out the API service call rather than using nock or trying to respond dynamically to the request. Example: // Example of failed API request cy.stub(Event, \"read\") .as(\"fetchEvent\") // Alias to access this request later .rejects(new Error(\"Request failed with status code 404\")); // Example of successful API request cy.stub(Event, \"read\") .withArgs(\"uuid\") // Specify expected arguments here .as(\"fetchEvent\") .resolves(eventStub); More detailed examples in following tests: AnalyzeAlertForm.spec.ts EventAlertsTable.spec.ts","title":"Mocking API Calls"},{"location":"development/frontend/testing/#testing-strategies","text":"Some things you might want to test might not be very intuitive using Cypress.","title":"Testing Strategies"},{"location":"development/frontend/testing/#testing-links","text":"If a component dynamically generates a link, you might want to test that said link was correctly generated and attached to the element. You can easily do this by doing the following, rather than clicking on the element and checking the resulting window location. cy.contains(\"Click Me to Go To New Page!\") .invoke(\"attr\", \"href\") .should(\"contain\", \"/my/page\"); // check for the dynamically generated link here","title":"Testing Links"},{"location":"development/frontend/testing/#testing-time-dependent-data","text":"Component tests are set-up to automatically run in the Eastern/New_York timezone, no matter where the tests are actually being ran from. Make sure to consider that as some datetimes are reformatted to be in the browser's timezone. In some cases, the current time will be auto-filled into forms. In order to check that these values are set correctly, cy.clock can be used to mock the current time, so that this value will be the same whenever tests are ran. Example: // Using UTC for the test time will make it easier to convert where necessary const testTime = new Date(Date.UTC(2022, 2, 29, 12, 0, 0, 0)).getTime(); describe(\"My Test Suite\", () => { it(\"My Test\", () => { cy.clock(testTime); // Rest of test... }); });","title":"Testing Time-Dependent Data"},{"location":"development/frontend/testing/#testing-for-child-components","text":"For the most part, you won't need to test for child components, however, in some cases you may want to access child components in order to verify that they were rendered. Example: EmailAnalysis.vue , is really just a container for its child components. Example of how to check for child components: import ChildComponent from \"@/components/ChildComponent.vue\"; import ParentComponent from \"@/components/ParentComponent.vue\"; mount(ParentComponent).then((wrapper) => { expect(wrapper.getComponent(ChildComponent)).to.exist; });","title":"Testing for Child Components"},{"location":"development/frontend/testing/#testing-for-event-reaction-and-emitted","text":"TBU (once I get better examples :))","title":"Testing for Event (reaction and emitted)"},{"location":"development/frontend/testing/#end-to-end-tests","text":"ToDo","title":"End-to-End Tests"},{"location":"usage/api/alert_filters/","text":"Alert Filters \u00b6 The API provides several ways to query for and filter alerts. Any of the filters can be combined with one another to produce complex queries. NOTE: If you combine multiple filters or use one of the filters that allows you to specify a comma-separated list of items, they are all queried for using AND logic. Disposition \u00b6 To fetch all alerts that were assigned a disposition of DELIVERY : /api/alert/?disposition=DELIVERY To fetch all alerts that have not yet been dispositioned: /api/alert/?disposition=none Disposition User \u00b6 To fetch all alerts that were dispositioned by the username bob : /api/alert/?disposition_user=bob Dispositioned After \u00b6 To fetch all alerts that were dispositioned after January 1, 2021: /api/alert/?dispositioned_after=2021-01-01+00:00:00.000000+00:00 NOTE: All timestamps are stored in the database using UTC. You will also need to ensure that the timestamp that you pass in is properly url-encoded. Dispositioned Before \u00b6 To fetch all alerts that were dispositioned before January 1, 2021: /api/alert/?dispositioned_before=2021-01-01+00:00:00.000000+00:00 NOTE: All timestamps are stored in the database using UTC. You will also need to ensure that the timestamp that you pass in is properly url-encoded. Event UUID \u00b6 To fetch all alerts that are associated with the event UUID 98dab2bf-2683-48e7-8193-183c3c5e4490 : /api/alert/?event_uuid=98dab2bf-2683-48e7-8193-183c3c5e4490 Event Time After \u00b6 To fetch all alerts with the event_time after January 1, 2021: /api/alert/?event_time_after=2021-01-01+00:00:00.000000+00:00 NOTE: All timestamps are stored in the database using UTC. You will also need to ensure that the timestamp that you pass in is properly url-encoded. Event Time Before \u00b6 To fetch all alerts that were event_time before January 1, 2021: /api/alert/?event_time_before=2021-01-01+00:00:00.000000+00:00 NOTE: All timestamps are stored in the database using UTC. You will also need to ensure that the timestamp that you pass in is properly url-encoded. Insert Time After \u00b6 To fetch all alerts with the insert_time after January 1, 2021: /api/alert/?insert_time_after=2021-01-01+00:00:00.000000+00:00 NOTE: All timestamps are stored in the database using UTC. You will also need to ensure that the timestamp that you pass in is properly url-encoded. Insert Time Before \u00b6 To fetch all alerts that were insert_time before January 1, 2021: /api/alert/?insert_time_before=2021-01-01+00:00:00.000000+00:00 NOTE: All timestamps are stored in the database using UTC. You will also need to ensure that the timestamp that you pass in is properly url-encoded. Name \u00b6 To fetch all alerts with asdf in their name: /api/alert/?name=asdf NOTE: This performs a substring match. Observable \u00b6 To fetch all alerts that contain a specific observable with the type of fqdn and value of google.com : /api/alert/?observable=fqdn|google.com Observable Types \u00b6 To fetch all alerts that contain observables with the types of fqdn and ip : /api/alert/?observable_types=fqdn,ip Observable Value \u00b6 To fetch all alerts that contain observables (regardless of their type) that have the value google.com : /api/alert/?observable_value=google.com Owner \u00b6 To fetch all alerts that are owned by the username bob : /api/alert/?owner=bob Queue \u00b6 To fetch all alerts that are inside of the alert queue intel : /api/alert/?queue=intel Tags \u00b6 To fetch all alerts that have the tags email and malicious : /api/alert/?tags=email,malicious Threat Actor \u00b6 To fetch all alerts that were assigned the threat actor Bad Guy : /api/alert/?threat_actor=Bad Guy Threats \u00b6 To fetch all alerts that were assigned the threats emotet and zbot : /api/alert/?threats=emotet,zbot Tool \u00b6 To fetch all alerts produced by the tool Splunk : /api/alert/?tool=Splunk Tool Instance \u00b6 To fetch all alerts produced by the tool instance splunkserver1 : /api/alert/?tool_instance=splunkserver1 Type \u00b6 To fetch all alerts of type SMTP : /api/alert/?type=SMTP Combining timestamp filters \u00b6 You can combine the various timestamp filters to get alerts from a single day. For example, to get alerts that were dispositioned during November 1, 2021: /api/alert/?dispositioned_after=2021-11-01+00:00:00.000000+00:00&dispositioned_before=2021-11-02+00:00:00.000000+00:00 NOTE: All timestamps are stored in the database using UTC. You will also need to ensure that the timestamp that you pass in is properly url-encoded.","title":"Alert Filters"},{"location":"usage/api/alert_filters/#alert-filters","text":"The API provides several ways to query for and filter alerts. Any of the filters can be combined with one another to produce complex queries. NOTE: If you combine multiple filters or use one of the filters that allows you to specify a comma-separated list of items, they are all queried for using AND logic.","title":"Alert Filters"},{"location":"usage/api/alert_filters/#disposition","text":"To fetch all alerts that were assigned a disposition of DELIVERY : /api/alert/?disposition=DELIVERY To fetch all alerts that have not yet been dispositioned: /api/alert/?disposition=none","title":"Disposition"},{"location":"usage/api/alert_filters/#disposition-user","text":"To fetch all alerts that were dispositioned by the username bob : /api/alert/?disposition_user=bob","title":"Disposition User"},{"location":"usage/api/alert_filters/#dispositioned-after","text":"To fetch all alerts that were dispositioned after January 1, 2021: /api/alert/?dispositioned_after=2021-01-01+00:00:00.000000+00:00 NOTE: All timestamps are stored in the database using UTC. You will also need to ensure that the timestamp that you pass in is properly url-encoded.","title":"Dispositioned After"},{"location":"usage/api/alert_filters/#dispositioned-before","text":"To fetch all alerts that were dispositioned before January 1, 2021: /api/alert/?dispositioned_before=2021-01-01+00:00:00.000000+00:00 NOTE: All timestamps are stored in the database using UTC. You will also need to ensure that the timestamp that you pass in is properly url-encoded.","title":"Dispositioned Before"},{"location":"usage/api/alert_filters/#event-uuid","text":"To fetch all alerts that are associated with the event UUID 98dab2bf-2683-48e7-8193-183c3c5e4490 : /api/alert/?event_uuid=98dab2bf-2683-48e7-8193-183c3c5e4490","title":"Event UUID"},{"location":"usage/api/alert_filters/#event-time-after","text":"To fetch all alerts with the event_time after January 1, 2021: /api/alert/?event_time_after=2021-01-01+00:00:00.000000+00:00 NOTE: All timestamps are stored in the database using UTC. You will also need to ensure that the timestamp that you pass in is properly url-encoded.","title":"Event Time After"},{"location":"usage/api/alert_filters/#event-time-before","text":"To fetch all alerts that were event_time before January 1, 2021: /api/alert/?event_time_before=2021-01-01+00:00:00.000000+00:00 NOTE: All timestamps are stored in the database using UTC. You will also need to ensure that the timestamp that you pass in is properly url-encoded.","title":"Event Time Before"},{"location":"usage/api/alert_filters/#insert-time-after","text":"To fetch all alerts with the insert_time after January 1, 2021: /api/alert/?insert_time_after=2021-01-01+00:00:00.000000+00:00 NOTE: All timestamps are stored in the database using UTC. You will also need to ensure that the timestamp that you pass in is properly url-encoded.","title":"Insert Time After"},{"location":"usage/api/alert_filters/#insert-time-before","text":"To fetch all alerts that were insert_time before January 1, 2021: /api/alert/?insert_time_before=2021-01-01+00:00:00.000000+00:00 NOTE: All timestamps are stored in the database using UTC. You will also need to ensure that the timestamp that you pass in is properly url-encoded.","title":"Insert Time Before"},{"location":"usage/api/alert_filters/#name","text":"To fetch all alerts with asdf in their name: /api/alert/?name=asdf NOTE: This performs a substring match.","title":"Name"},{"location":"usage/api/alert_filters/#observable","text":"To fetch all alerts that contain a specific observable with the type of fqdn and value of google.com : /api/alert/?observable=fqdn|google.com","title":"Observable"},{"location":"usage/api/alert_filters/#observable-types","text":"To fetch all alerts that contain observables with the types of fqdn and ip : /api/alert/?observable_types=fqdn,ip","title":"Observable Types"},{"location":"usage/api/alert_filters/#observable-value","text":"To fetch all alerts that contain observables (regardless of their type) that have the value google.com : /api/alert/?observable_value=google.com","title":"Observable Value"},{"location":"usage/api/alert_filters/#owner","text":"To fetch all alerts that are owned by the username bob : /api/alert/?owner=bob","title":"Owner"},{"location":"usage/api/alert_filters/#queue","text":"To fetch all alerts that are inside of the alert queue intel : /api/alert/?queue=intel","title":"Queue"},{"location":"usage/api/alert_filters/#tags","text":"To fetch all alerts that have the tags email and malicious : /api/alert/?tags=email,malicious","title":"Tags"},{"location":"usage/api/alert_filters/#threat-actor","text":"To fetch all alerts that were assigned the threat actor Bad Guy : /api/alert/?threat_actor=Bad Guy","title":"Threat Actor"},{"location":"usage/api/alert_filters/#threats","text":"To fetch all alerts that were assigned the threats emotet and zbot : /api/alert/?threats=emotet,zbot","title":"Threats"},{"location":"usage/api/alert_filters/#tool","text":"To fetch all alerts produced by the tool Splunk : /api/alert/?tool=Splunk","title":"Tool"},{"location":"usage/api/alert_filters/#tool-instance","text":"To fetch all alerts produced by the tool instance splunkserver1 : /api/alert/?tool_instance=splunkserver1","title":"Tool Instance"},{"location":"usage/api/alert_filters/#type","text":"To fetch all alerts of type SMTP : /api/alert/?type=SMTP","title":"Type"},{"location":"usage/api/alert_filters/#combining-timestamp-filters","text":"You can combine the various timestamp filters to get alerts from a single day. For example, to get alerts that were dispositioned during November 1, 2021: /api/alert/?dispositioned_after=2021-11-01+00:00:00.000000+00:00&dispositioned_before=2021-11-02+00:00:00.000000+00:00 NOTE: All timestamps are stored in the database using UTC. You will also need to ensure that the timestamp that you pass in is properly url-encoded.","title":"Combining timestamp filters"},{"location":"usage/api/alert_sorting/","text":"Alert Sorting \u00b6 The API provides several ways to sort the alerts that are returned by the /api/alert/ endpoint. Disposition \u00b6 To sort the alerts by their disposition: Ascending /api/alert/?sort=disposition|asc Descending /api/alert/?sort=disposition|desc NOTE: Sorting by disposition will be skipped if you are also filtering the alerts by disposition. Disposition Time \u00b6 To sort the alerts by their disposition time: Ascending /api/alert/?sort=disposition_time|asc Descending /api/alert/?sort=disposition_time|desc Disposition User \u00b6 To sort the alerts by their disposition user: Ascending /api/alert/?sort=disposition_user|asc Descending /api/alert/?sort=disposition_user|desc NOTE: Sorting by disposition user will be skipped if you are also filtering the alerts by disposition user. Event Time \u00b6 To sort the alerts by their event time: Ascending /api/alert/?sort=event_time|asc Descending /api/alert/?sort=event_time|desc Insert Time \u00b6 To sort the alerts by their insert time: Ascending /api/alert/?sort=insert_time|asc Descending /api/alert/?sort=insert_time|desc Name \u00b6 To sort the alerts by their name: Ascending /api/alert/?sort=name|asc Descending /api/alert/?sort=name|desc Owner \u00b6 To sort the alerts by their owner: Ascending /api/alert/?sort=owner|asc Descending /api/alert/?sort=owner|desc NOTE: Sorting by owner will be skipped if you are also filtering the alerts by owner. Queue \u00b6 To sort the alerts by their queue: Ascending /api/alert/?sort=queue|asc Descending /api/alert/?sort=queue|desc NOTE: Sorting by queue will be skipped if you are also filtering the alerts by queue. Type \u00b6 To sort the alerts by their type: Ascending /api/alert/?sort=type|asc Descending /api/alert/?sort=type|desc NOTE: Sorting by type will be skipped if you are also filtering the alerts by type.","title":"Alert Sorting"},{"location":"usage/api/alert_sorting/#alert-sorting","text":"The API provides several ways to sort the alerts that are returned by the /api/alert/ endpoint.","title":"Alert Sorting"},{"location":"usage/api/alert_sorting/#disposition","text":"To sort the alerts by their disposition: Ascending /api/alert/?sort=disposition|asc Descending /api/alert/?sort=disposition|desc NOTE: Sorting by disposition will be skipped if you are also filtering the alerts by disposition.","title":"Disposition"},{"location":"usage/api/alert_sorting/#disposition-time","text":"To sort the alerts by their disposition time: Ascending /api/alert/?sort=disposition_time|asc Descending /api/alert/?sort=disposition_time|desc","title":"Disposition Time"},{"location":"usage/api/alert_sorting/#disposition-user","text":"To sort the alerts by their disposition user: Ascending /api/alert/?sort=disposition_user|asc Descending /api/alert/?sort=disposition_user|desc NOTE: Sorting by disposition user will be skipped if you are also filtering the alerts by disposition user.","title":"Disposition User"},{"location":"usage/api/alert_sorting/#event-time","text":"To sort the alerts by their event time: Ascending /api/alert/?sort=event_time|asc Descending /api/alert/?sort=event_time|desc","title":"Event Time"},{"location":"usage/api/alert_sorting/#insert-time","text":"To sort the alerts by their insert time: Ascending /api/alert/?sort=insert_time|asc Descending /api/alert/?sort=insert_time|desc","title":"Insert Time"},{"location":"usage/api/alert_sorting/#name","text":"To sort the alerts by their name: Ascending /api/alert/?sort=name|asc Descending /api/alert/?sort=name|desc","title":"Name"},{"location":"usage/api/alert_sorting/#owner","text":"To sort the alerts by their owner: Ascending /api/alert/?sort=owner|asc Descending /api/alert/?sort=owner|desc NOTE: Sorting by owner will be skipped if you are also filtering the alerts by owner.","title":"Owner"},{"location":"usage/api/alert_sorting/#queue","text":"To sort the alerts by their queue: Ascending /api/alert/?sort=queue|asc Descending /api/alert/?sort=queue|desc NOTE: Sorting by queue will be skipped if you are also filtering the alerts by queue.","title":"Queue"},{"location":"usage/api/alert_sorting/#type","text":"To sort the alerts by their type: Ascending /api/alert/?sort=type|asc Descending /api/alert/?sort=type|desc NOTE: Sorting by type will be skipped if you are also filtering the alerts by type.","title":"Type"}]}